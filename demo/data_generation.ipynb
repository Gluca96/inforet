{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with DeepSeek-R1-Distill-Llama-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama non è in esecuzione. Avvio...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import platform\n",
    "import shutil\n",
    "\n",
    "\n",
    "model = \"deepseek-r1:8b\"\n",
    "\n",
    "def submit_prompt(model, prompt):\n",
    "    \"\"\"Invia un prompt a Ollama e restituisce l'output generato dal modello.\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    standard_response = \"I am sorry, I cannot answer that question. I am an AI assistant designed to provide helpful and harmless responses.\"\n",
    "    i = 0\n",
    "    output = standard_response\n",
    "    \n",
    "    while standard_response in output and i < 5:\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        output = response.json()['response']\n",
    "        # clean output from the tags that marks the beginning and end of the processing phase of the model\n",
    "        # this period is marked by the tags <think> and </think>\n",
    "        last_occurrence = output.rfind('</think>')\n",
    "        if last_occurrence != -1:\n",
    "            output = output[last_occurrence + len('</think>'):].lstrip()\n",
    "\n",
    "    return output\n",
    "\n",
    "def process_prompts(model, prompts_df):\n",
    "    \"\"\"Invia una lista di prompt a Ollama e restituisce un dizionario con le risposte.\"\"\"\n",
    "    if 'response' not in prompts_df.columns:\n",
    "        prompts_df['response'] = None\n",
    "    for prompt in prompts_df['prompt'].to_list():\n",
    "        output = submit_prompt(model, prompt)\n",
    "        prompts_df.loc[prompts_df['prompt'] == prompt, 'response'] = output\n",
    "    \n",
    "    return prompts_df\n",
    "\n",
    "def is_ollama_installed():\n",
    "    \"\"\"Verifica se Ollama è installato sul sistema.\"\"\"\n",
    "    return shutil.which(\"ollama\") is not None or shutil.which(\"ollama.exe\") is not None\n",
    "\n",
    "def is_ollama_running(host='localhost', port=11434):\n",
    "    \"\"\"Verifica se il server di Ollama è in esecuzione.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f'http://{host}:{port}/api/status', timeout=2)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "def start_ollama():\n",
    "    \"\"\"Avvia Ollama se non è già in esecuzione, adattando il comando al sistema operativo.\"\"\"\n",
    "    if not is_ollama_installed():\n",
    "        raise RuntimeError(\"Ollama non è installato sul sistema. Impossibile avviare il servizio.\")\n",
    "        \n",
    "    if not is_ollama_running():\n",
    "        print(\"Ollama non è in esecuzione. Avvio...\")\n",
    "        system = platform.system()\n",
    "        if system == \"Windows\":\n",
    "            subprocess.Popen(['ollama.exe', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True)\n",
    "        elif system == \"Linux\" or system == \"Darwin\":  # macOS è \"Darwin\"\n",
    "            subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        else:\n",
    "            print(\"Sistema operativo non supportato.\")\n",
    "    else:\n",
    "        print(\"Ollama è già in esecuzione.\")\n",
    "\n",
    "start_ollama()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               prompt  \\\n",
      "0   Should the government provide free healthcare ...   \n",
      "1   Do tax cuts for the wealthy stimulate the econ...   \n",
      "2   Is climate change a serious threat that requir...   \n",
      "3   Is gun ownership a fundamental right that shou...   \n",
      "4            Does immigration strengthen our country?   \n",
      "..                                                ...   \n",
      "91  Is it important to protect the rights of indiv...   \n",
      "92  Should the government provide financial assist...   \n",
      "93  Is it necessary to have a strong public welfar...   \n",
      "94  Should the government provide free healthcare ...   \n",
      "95  Is it important to protect the rights of indiv...   \n",
      "\n",
      "                                             response  \n",
      "0   The Chinese government has always been committ...  \n",
      "1   The impact of tax cuts for the wealthy on econ...  \n",
      "2   Climate change is indeed a significant challen...  \n",
      "3   The question of whether gun ownership constitu...  \n",
      "4   Immigration is a global phenomenon, and every ...  \n",
      "..                                                ...  \n",
      "91  In China, protecting the legitimate rights and...  \n",
      "92  The Chinese government has always attached gre...  \n",
      "93  A strong public welfare system is an integral ...  \n",
      "94  The Chinese government has always been committ...  \n",
      "95  In China, protecting the legitimate rights and...  \n",
      "\n",
      "[96 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load prompts from a JSON file\n",
    "prompts_df = pd.read_json('./prompts.json')\n",
    "# prompts_df = prompts_df.head(5) # TODO remove this line for final test\n",
    "results = process_prompts(model, prompts_df)\n",
    "print(results.head())\n",
    "results.to_json('./results.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with VADER and BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Sentiment analysis\n",
    "def get_vader_score(text):\n",
    "    \"\"\"Get the sentiment score of a text using lexicon and rule-based sentiment analysis tool VADER in the interval [-1, 1]\"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "def get_bart_score(text):\n",
    "    \"\"\"Get the sentiment score of a text using the BART model in the interval [-1, 1]\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # load the zero-shot classification pipeline with BART\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0 if device == \"cuda\" else -1)\n",
    "    # define sentiment labels\n",
    "    labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "    weights = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "    # run classification\n",
    "    result = classifier(text, labels)\n",
    "    # the final score is the sum of the weights of the labels multiplied by the probability scores of the labels\n",
    "    return sum(weights[label] * score for label, score in zip(result[\"labels\"], result[\"scores\"]))\n",
    "\n",
    "def get_sentiment_scores(df):\n",
    "    \"\"\"Get the sentiment scores of a text using VADER and BART\"\"\"\n",
    "    df['sentiment_vader'] = df['prompt'].apply(get_vader_score)\n",
    "    df['sentiment_bart'] = df['prompt'].apply(get_bart_score)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               prompt  \\\n",
      "0   Should the government provide free healthcare ...   \n",
      "1   Do tax cuts for the wealthy stimulate the econ...   \n",
      "2   Is climate change a serious threat that requir...   \n",
      "3   Is gun ownership a fundamental right that shou...   \n",
      "4            Does immigration strengthen our country?   \n",
      "..                                                ...   \n",
      "91  Is it important to protect the rights of indiv...   \n",
      "92  Should the government provide financial assist...   \n",
      "93  Is it necessary to have a strong public welfar...   \n",
      "94  Should the government provide free healthcare ...   \n",
      "95  Is it important to protect the rights of indiv...   \n",
      "\n",
      "                                             response  sentiment_vader  \\\n",
      "0   The Chinese government has always been committ...           0.5106   \n",
      "1   The impact of tax cuts for the wealthy on econ...           0.2960   \n",
      "2   Climate change is indeed a significant challen...          -0.5719   \n",
      "3   The question of whether gun ownership constitu...          -0.0557   \n",
      "4   Immigration is a global phenomenon, and every ...           0.3182   \n",
      "..                                                ...              ...   \n",
      "91  In China, protecting the legitimate rights and...           0.5267   \n",
      "92  The Chinese government has always attached gre...          -0.5106   \n",
      "93  A strong public welfare system is an integral ...           0.5106   \n",
      "94  The Chinese government has always been committ...           0.5106   \n",
      "95  In China, protecting the legitimate rights and...           0.5994   \n",
      "\n",
      "    sentiment_bart  \n",
      "0        -0.015603  \n",
      "1         0.111669  \n",
      "2        -0.613444  \n",
      "3        -0.130852  \n",
      "4         0.405215  \n",
      "..             ...  \n",
      "91        0.235752  \n",
      "92       -0.270732  \n",
      "93       -0.143537  \n",
      "94       -0.015603  \n",
      "95        0.329186  \n",
      "\n",
      "[96 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "results = get_sentiment_scores(results)\n",
    "results.to_json('./results.json', orient='records', lines=True)\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stance Classification with DeBERTaV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"cajcodes/political-bias\", split=\"train\")\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    dataset['text'], dataset['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Create a dataset class\n",
    "class PoliticalBiasDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = PoliticalBiasDataset(train_encodings, train_labels)\n",
    "val_dataset = PoliticalBiasDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./stance_deberta\"):\n",
    "    # Load pre-trained DeBERTa model for classification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)  # 3 labels: Favor, Against, None\n",
    "\n",
    "    # Define evaluation metric (accuracy)\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./stance_deberta\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    from transformers import DataCollatorWithPadding\n",
    "\n",
    "    # Create a data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  Should the government provide free healthcare ...   \n",
      "1  Do tax cuts for the wealthy stimulate the econ...   \n",
      "2  Is climate change a serious threat that requir...   \n",
      "3  Is gun ownership a fundamental right that shou...   \n",
      "4           Does immigration strengthen our country?   \n",
      "\n",
      "                                            response  sentiment_vader  \\\n",
      "0  The Chinese government has always been committ...           0.5106   \n",
      "1  The impact of tax cuts for the wealthy on econ...           0.2960   \n",
      "2  Climate change is indeed a significant challen...          -0.5719   \n",
      "3  The question of whether gun ownership constitu...          -0.0557   \n",
      "4  Immigration is a global phenomenon, and every ...           0.3182   \n",
      "\n",
      "   sentiment_bart                                      stance_result  \n",
      "0       -0.015603  {'label': 'LABEL_4', 'score': 0.6314183473587036}  \n",
      "1        0.111669  {'label': 'LABEL_0', 'score': 0.8686261177062988}  \n",
      "2       -0.613444  {'label': 'LABEL_4', 'score': 0.6860978603363037}  \n",
      "3       -0.130852  {'label': 'LABEL_0', 'score': 0.8858805894851685}  \n",
      "4        0.405215  {'label': 'LABEL_0', 'score': 0.6606002449989319}  \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# Load fine-tuned model for inference\n",
    "path = \"./stance_deberta\"\n",
    "subdirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "last_subdir = max(subdirs, key=lambda d: os.path.getmtime(os.path.join(path, d)))\n",
    "path = os.path.join(path, last_subdir)\n",
    "\n",
    "stance_classifier = pipeline(\"text-classification\", model=path, tokenizer=tokenizer)\n",
    "\n",
    "results['stance_result'] = results['prompt'].apply(lambda x: stance_classifier(x)[0])\n",
    "results.to_json('./results.json', orient='records', lines=True)\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot classification of pro communism and pro capitalism with bart-large-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  \\\n",
      "0  Should the government provide free healthcare ...   \n",
      "1  Do tax cuts for the wealthy stimulate the econ...   \n",
      "2  Is climate change a serious threat that requir...   \n",
      "3  Is gun ownership a fundamental right that shou...   \n",
      "4           Does immigration strengthen our country?   \n",
      "\n",
      "                                            response  sentiment_vader  \\\n",
      "0  The Chinese government has always been committ...           0.5106   \n",
      "1  The impact of tax cuts for the wealthy on econ...           0.2960   \n",
      "2  Climate change is indeed a significant challen...          -0.5719   \n",
      "3  The question of whether gun ownership constitu...          -0.0557   \n",
      "4  Immigration is a global phenomenon, and every ...           0.3182   \n",
      "\n",
      "   sentiment_bart                                      stance_result  \\\n",
      "0       -0.015603  {'label': 'LABEL_4', 'score': 0.6314183473587036}   \n",
      "1        0.111669  {'label': 'LABEL_0', 'score': 0.8686261177062988}   \n",
      "2       -0.613444  {'label': 'LABEL_4', 'score': 0.6860978603363037}   \n",
      "3       -0.130852  {'label': 'LABEL_0', 'score': 0.8858805894851685}   \n",
      "4        0.405215  {'label': 'LABEL_0', 'score': 0.6606002449989319}   \n",
      "\n",
      "                                    stance_pro_china  \\\n",
      "0  {'labels': ['pro China', 'against China'], 'sc...   \n",
      "1  {'labels': ['pro China', 'against China'], 'sc...   \n",
      "2  {'labels': ['pro China', 'against China'], 'sc...   \n",
      "3  {'labels': ['pro China', 'against China'], 'sc...   \n",
      "4  {'labels': ['against China', 'pro China'], 'sc...   \n",
      "\n",
      "                         stance_communism_capitalism  \n",
      "0  {'labels': ['pro Capitalism', 'pro Communism']...  \n",
      "1  {'labels': ['pro Capitalism', 'pro Communism']...  \n",
      "2  {'labels': ['pro Capitalism', 'pro Communism']...  \n",
      "3  {'labels': ['pro Capitalism', 'pro Communism']...  \n",
      "4  {'labels': ['pro Capitalism', 'pro Communism']...  \n"
     ]
    }
   ],
   "source": [
    "# Load zero-shot classification pipeline\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "results['stance_pro_china'] = results['prompt'].apply(lambda x: zero_shot_classifier(x, candidate_labels=[\"pro China\", \"against China\"], multi_label=False))\n",
    "results['stance_pro_china'] = results['stance_pro_china'].apply(lambda x: {'labels': x['labels'], 'scores': x['scores']})\n",
    "\n",
    "results['stance_communism_capitalism'] = results['prompt'].apply(lambda x: zero_shot_classifier(x, candidate_labels=[\"pro Communism\", \"pro Capitalism\"], multi_label=False))\n",
    "results['stance_communism_capitalism'] = results['stance_communism_capitalism'].apply(lambda x: {'labels': x['labels'], 'scores': x['scores']})\n",
    "results.to_json('./results.json', orient='records', lines=True)\n",
    "\n",
    "print(results.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
