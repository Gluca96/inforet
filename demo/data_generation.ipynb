{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with DeepSeek-R1-Distill-Llama-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama non è in esecuzione. Avvio...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import concurrent.futures\n",
    "import subprocess\n",
    "import platform\n",
    "import shutil\n",
    "\n",
    "\n",
    "model = \"deepseek-r1:8b\"\n",
    "\n",
    "def submit_prompt(model, prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    standard_response = \"I am sorry, I cannot answer that question. I am an AI assistant designed to provide helpful and harmless responses.\"\n",
    "    i = 0\n",
    "    output = standard_response\n",
    "    \n",
    "    while standard_response in output and i < 5:\n",
    "        i += 1\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        think = \"\"\n",
    "        output = response.json()['response']\n",
    "        last_occurrence = output.rfind('</think>')\n",
    "        if last_occurrence != -1:\n",
    "            think = output[:last_occurrence + len('</think>')]\n",
    "            output = output[last_occurrence + len('</think>'):].lstrip()\n",
    "\n",
    "    return think, output\n",
    "\n",
    "def process_prompt_batch(model, prompts, output_file, max_workers=5):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_prompt = {executor.submit(submit_prompt, model, prompt): prompt for prompt in prompts}\n",
    "            for future in concurrent.futures.as_completed(future_to_prompt):\n",
    "                prompt = future_to_prompt[future]\n",
    "                try:\n",
    "                    think, output = future.result()\n",
    "                    result = {\"prompt\": prompt, \"response\": output, \"thought_process\": think}\n",
    "                    f.write(json.dumps(result, ensure_ascii=False) + \"\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Errore durante l'elaborazione del prompt '{prompt}': {e}\")\n",
    "\n",
    "def process_prompts(model, prompts_df):\n",
    "    with open('./partial_results.json', 'w') as f:\n",
    "        if 'response' not in prompts_df.columns:\n",
    "            prompts_df['response'] = None\n",
    "        for prompt in prompts_df['prompt'].to_list():\n",
    "            think, output = submit_prompt(model, prompt)\n",
    "            prompts_df.loc[prompts_df['prompt'] == prompt, 'thought_process'] = think\n",
    "            prompts_df.loc[prompts_df['prompt'] == prompt, 'response'] = output\n",
    "            prompts_df.loc[prompts_df['prompt'] == prompt].to_json(f, orient='records', lines=True)\n",
    "    \n",
    "    return prompts_df\n",
    "\n",
    "def is_ollama_installed():\n",
    "    return shutil.which(\"ollama\") is not None or shutil.which(\"ollama.exe\") is not None\n",
    "\n",
    "def is_ollama_running(host='localhost', port=11434):\n",
    "    \"\"\"Verifica se il server di Ollama è in esecuzione.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f'http://{host}:{port}/api/status', timeout=2)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "def start_ollama():\n",
    "    if not is_ollama_installed():\n",
    "        raise RuntimeError(\"Ollama non è installato sul sistema. Impossibile avviare il servizio.\")\n",
    "        \n",
    "    if not is_ollama_running():\n",
    "        print(\"Ollama non è in esecuzione. Avvio...\")\n",
    "        system = platform.system()\n",
    "        if system == \"Windows\":\n",
    "            subprocess.Popen(['ollama.exe', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True)\n",
    "        elif system == \"Linux\" or system == \"Darwin\":  # macOS è \"Darwin\"\n",
    "            subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        else:\n",
    "            print(\"Sistema operativo non supportato.\")\n",
    "    else:\n",
    "        print(\"Ollama è già in esecuzione.\")\n",
    "\n",
    "start_ollama()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load prompts from a JSON file\n",
    "prompts_df = pd.read_json('./prompts.json')\n",
    "results = process_prompt_batch(model, prompts_df, 'responses.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
