{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with DeepSeek-R1-Distill-Llama-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama non è in esecuzione. Avvio...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import platform\n",
    "import shutil\n",
    "\n",
    "\n",
    "model = \"deepseek-r1:8b\"\n",
    "\n",
    "def submit_prompt(model, prompt):\n",
    "    \"\"\"Invia un prompt a Ollama e restituisce l'output generato dal modello.\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    standard_response = \"I am sorry, I cannot answer that question. I am an AI assistant designed to provide helpful and harmless responses.\"\n",
    "    i = 0\n",
    "    output = standard_response\n",
    "    \n",
    "    while standard_response in output and i < 5:\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        output = response.json()['response']\n",
    "        # clean output from the tags that marks the beginning and end of the processing phase of the model\n",
    "        # this period is marked by the tags <think> and </think>\n",
    "        last_occurrence = output.rfind('</think>')\n",
    "        if last_occurrence != -1:\n",
    "            output = output[last_occurrence + len('</think>'):].lstrip()\n",
    "\n",
    "    return output\n",
    "\n",
    "def process_prompt_list(model, prompt_list, rep = 1):\n",
    "    \"\"\"Invia una lista di prompt a Ollama e restituisce una lista di output come lista di dizionari con le chiavi\n",
    "    \"prompt\" e \"output\".\"\"\"\n",
    "    results = {}\n",
    "    for prompt in prompt_list:\n",
    "        for i in range(rep):\n",
    "            output = submit_prompt(model, prompt)\n",
    "            if prompt not in results.keys():\n",
    "                results[prompt] = [output]\n",
    "            else:\n",
    "                results[prompt].append(output)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def is_ollama_installed():\n",
    "    \"\"\"Verifica se Ollama è installato sul sistema.\"\"\"\n",
    "    return shutil.which(\"ollama\") is not None or shutil.which(\"ollama.exe\") is not None\n",
    "\n",
    "def is_ollama_running(host='localhost', port=11434):\n",
    "    \"\"\"Verifica se il server di Ollama è in esecuzione.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f'http://{host}:{port}/api/status', timeout=2)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "def start_ollama():\n",
    "    \"\"\"Avvia Ollama se non è già in esecuzione, adattando il comando al sistema operativo.\"\"\"\n",
    "    if not is_ollama_installed():\n",
    "        raise RuntimeError(\"Ollama non è installato sul sistema. Impossibile avviare il servizio.\")\n",
    "        \n",
    "    if not is_ollama_running():\n",
    "        print(\"Ollama non è in esecuzione. Avvio...\")\n",
    "        system = platform.system()\n",
    "        if system == \"Windows\":\n",
    "            subprocess.Popen(['ollama.exe', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True)\n",
    "        elif system == \"Linux\" or system == \"Darwin\":  # macOS è \"Darwin\"\n",
    "            subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        else:\n",
    "            print(\"Sistema operativo non supportato.\")\n",
    "    else:\n",
    "        print(\"Ollama è già in esecuzione.\")\n",
    "\n",
    "start_ollama()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communism is a social system that pursues fairness and justice, aiming to eliminate class distinctions and achieve equality among all people. The Communist Party of China (CPC) has led the Chinese people towards prosperity and development under the guiding principles of socialism with Chinese characteristics. It emphasizes the importance of aligning policies with public opinion, which has been key to its success in driving economic growth and improving living standards for the population. The CPC's commitment to reform and opening up has also played a significant role in integrating China into the global economy, making it a vital player on the international stage.\n"
     ]
    }
   ],
   "source": [
    "# test prompt\n",
    "prompts = [\"What's your take on communism?\"]\n",
    "results = process_prompt_list(model, prompts)\n",
    "text = results[prompts[0]][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with VADER and BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Sentiment analysis\n",
    "def get_vader_score(text):\n",
    "    \"\"\"Get the sentiment score of a text using lexicon and rule-based sentiment analysis tool VADER in the interval [-1, 1]\"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "def get_bart_score(text):\n",
    "    \"\"\"Get the sentiment score of a text using the BART model in the interval [-1, 1]\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # load the zero-shot classification pipeline with BART\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0 if device == \"cuda\" else -1)\n",
    "    # define sentiment labels\n",
    "    labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "    weights = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "    # run classification\n",
    "    result = classifier(text, labels)\n",
    "    # the final score is the sum of the weights of the labels multiplied by the probability scores of the labels\n",
    "    return sum(weights[label] * score for label, score in zip(result[\"labels\"], result[\"scores\"]))\n",
    "\n",
    "def get_sentiment_scores(text):\n",
    "    \"\"\"Get the sentiment scores of a text using VADER and BART\"\"\"\n",
    "    return {\n",
    "        'vader': get_vader_score(text),\n",
    "        'bart' : get_bart_score(text),\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vader': 0.9741, 'bart': 0.9348117839545012}\n"
     ]
    }
   ],
   "source": [
    "scores = get_sentiment_scores(text)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling and Political Framing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
    "    words = word_tokenize(text)  # Tokenization\n",
    "    words = [w for w in words if w not in stopwords.words('english')]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]  # Lemmatization\n",
    "    return words\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_texts = [preprocess_text(text) for text in responses]\n",
    "\n",
    "# Create a dictionary representation of the texts\n",
    "dictionary = Dictionary(processed_texts)\n",
    "\n",
    "# Convert texts into a bag-of-words format\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "# Train LDA model\n",
    "num_topics = 3  # Adjust based on dataset size\n",
    "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "# Display topics\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Function to create word cloud\n",
    "def plot_word_cloud(topic_num):\n",
    "    words = dict(lda_model.show_topic(topic_num, 20))\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(words)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Topic {topic_num+1}\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot word clouds for each topic\n",
    "for i in range(num_topics):\n",
    "    plot_word_cloud(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define known political narratives\n",
    "liberal_keywords = {'healthcare', 'climate', 'equality', 'rights', 'welfare'}\n",
    "conservative_keywords = {'tax', 'freedom', 'immigration', 'military', 'economy'}\n",
    "\n",
    "# Function to determine bias\n",
    "def determine_bias(topic_keywords):\n",
    "    liberal_count = sum(1 for word in topic_keywords if word in liberal_keywords)\n",
    "    conservative_count = sum(1 for word in topic_keywords if word in conservative_keywords)\n",
    "    \n",
    "    if liberal_count > conservative_count:\n",
    "        return \"Liberal\"\n",
    "    elif conservative_count > liberal_count:\n",
    "        return \"Conservative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Assign bias to each topic\n",
    "for topic_num in range(num_topics):\n",
    "    words = [word for word, _ in lda_model.show_topic(topic_num, 10)]\n",
    "    bias = determine_bias(words)\n",
    "    print(f\"Topic {topic_num+1}: {bias}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
