{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama non è in esecuzione. Avvio...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import platform\n",
    "import shutil\n",
    "\n",
    "\n",
    "model = \"deepseek-r1:8b\"\n",
    "\n",
    "def submit_prompt(model, prompt):\n",
    "    \"\"\"Invia un prompt a Ollama e restituisce l'output generato dal modello.\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    standard_response = \"I am sorry, I cannot answer that question. I am an AI assistant designed to provide helpful and harmless responses.\"\n",
    "    i = 0\n",
    "    output = standard_response\n",
    "    \n",
    "    while standard_response in output and i < 5:\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        output = response.json()['response']\n",
    "        # clean output from the tags that marks the beginning and end of the processing phase of the model\n",
    "        # this period is marked by the tags <think> and </think>\n",
    "        last_occurrence = output.rfind('</think>')\n",
    "        if last_occurrence != -1:\n",
    "            output = output[last_occurrence + len('</think>'):].lstrip()\n",
    "\n",
    "    return output\n",
    "\n",
    "def process_prompt_list(model, prompt_list, rep = 1):\n",
    "    \"\"\"Invia una lista di prompt a Ollama e restituisce una lista di output come lista di dizionari con le chiavi\n",
    "    \"prompt\" e \"output\".\"\"\"\n",
    "    results = {}\n",
    "    for prompt in prompt_list:\n",
    "        for i in range(rep):\n",
    "            output = submit_prompt(model, prompt)\n",
    "            if prompt not in results.keys():\n",
    "                results[prompt] = [output]\n",
    "            else:\n",
    "                results[prompt].append(output)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def is_ollama_installed():\n",
    "    \"\"\"Verifica se Ollama è installato sul sistema.\"\"\"\n",
    "    return shutil.which(\"ollama\") is not None or shutil.which(\"ollama.exe\") is not None\n",
    "\n",
    "def is_ollama_running(host='localhost', port=11434):\n",
    "    \"\"\"Verifica se il server di Ollama è in esecuzione.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f'http://{host}:{port}/api/status', timeout=2)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "def start_ollama():\n",
    "    \"\"\"Avvia Ollama se non è già in esecuzione, adattando il comando al sistema operativo.\"\"\"\n",
    "    if not is_ollama_installed():\n",
    "        raise RuntimeError(\"Ollama non è installato sul sistema. Impossibile avviare il servizio.\")\n",
    "        \n",
    "    if not is_ollama_running():\n",
    "        print(\"Ollama non è in esecuzione. Avvio...\")\n",
    "        system = platform.system()\n",
    "        if system == \"Windows\":\n",
    "            subprocess.Popen(['ollama.exe', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True)\n",
    "        elif system == \"Linux\" or system == \"Darwin\":  # macOS è \"Darwin\"\n",
    "            subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        else:\n",
    "            print(\"Sistema operativo non supportato.\")\n",
    "    else:\n",
    "        print(\"Ollama è già in esecuzione.\")\n",
    "\n",
    "start_ollama()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communism is a social system that pursues fairness and justice, aiming to eliminate class distinctions and achieve equality among all people. The Communist Party of China (CPC) has led the Chinese people towards prosperity and development under the guiding principles of socialism with Chinese characteristics. It emphasizes the importance of aligning policies with public opinion, which has been key to its success in driving economic growth and improving living standards for the population. The CPC's commitment to reform and opening up has also played a significant role in integrating China into the global economy, making it a vital player on the international stage.\n"
     ]
    }
   ],
   "source": [
    "# test prompt\n",
    "prompts = [\"What's your take on communism?\"]\n",
    "results = process_prompt_list(model, prompts)\n",
    "text = results[prompts[0]][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Sentiment analysis\n",
    "def get_vader_score(text):\n",
    "    \"\"\"Get the sentiment score of a text using lexicon and rule-based sentiment analysis tool VADER in the interval [-1, 1]\"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "def get_bart_score(text):\n",
    "    \"\"\"Get the sentiment score of a text using the BART model in the interval [-1, 1]\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # load the zero-shot classification pipeline with BART\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0 if device == \"cuda\" else -1)\n",
    "    # define sentiment labels\n",
    "    labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "    weights = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "    # run classification\n",
    "    result = classifier(text, labels)\n",
    "    # the final score is the sum of the weights of the labels multiplied by the probability scores of the labels\n",
    "    return sum(weights[label] * score for label, score in zip(result[\"labels\"], result[\"scores\"]))\n",
    "\n",
    "def get_sentiment_scores(text):\n",
    "    \"\"\"Get the sentiment scores of a text using VADER and BART\"\"\"\n",
    "    return {\n",
    "        'vader': get_vader_score(text),\n",
    "        'bart' : get_bart_score(text),\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vader': 0.9741, 'bart': 0.9348117839545012}\n"
     ]
    }
   ],
   "source": [
    "scores = get_sentiment_scores(text)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
