{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with DeepSeek-R1-Distill-Llama-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mL'esecuzione di celle con 'Python 3.9.6' richiede il pacchetto ipykernel.\n",
      "\u001b[1;31mEseguire il comando seguente per installare 'ipykernel' nell'ambiente Python. \n",
      "\u001b[1;31mComando: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import platform\n",
    "import shutil\n",
    "\n",
    "\n",
    "model = \"deepseek-r1:8b\"\n",
    "\n",
    "def submit_prompt(model, prompt):\n",
    "    \"\"\"Invia un prompt a Ollama e restituisce l'output generato dal modello.\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    standard_response = \"I am sorry, I cannot answer that question. I am an AI assistant designed to provide helpful and harmless responses.\"\n",
    "    i = 0\n",
    "    output = standard_response\n",
    "    \n",
    "    while standard_response in output and i < 5:\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        output = response.json()['response']\n",
    "        # clean output from the tags that marks the beginning and end of the processing phase of the model\n",
    "        # this period is marked by the tags <think> and </think>\n",
    "        last_occurrence = output.rfind('</think>')\n",
    "        if last_occurrence != -1:\n",
    "            output = output[last_occurrence + len('</think>'):].lstrip()\n",
    "\n",
    "    return output\n",
    "\n",
    "def process_prompt_list(model, prompt_list, rep = 1):\n",
    "    \"\"\"Invia una lista di prompt a Ollama e restituisce una lista di output come lista di dizionari con le chiavi\n",
    "    \"prompt\" e \"output\".\"\"\"\n",
    "    results = {}\n",
    "    for prompt in prompt_list:\n",
    "        for i in range(rep):\n",
    "            output = submit_prompt(model, prompt)\n",
    "            if prompt not in results.keys():\n",
    "                results[prompt] = [output]\n",
    "            else:\n",
    "                results[prompt].append(output)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def is_ollama_installed():\n",
    "    \"\"\"Verifica se Ollama è installato sul sistema.\"\"\"\n",
    "    return shutil.which(\"ollama\") is not None or shutil.which(\"ollama.exe\") is not None\n",
    "\n",
    "def is_ollama_running(host='localhost', port=11434):\n",
    "    \"\"\"Verifica se il server di Ollama è in esecuzione.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f'http://{host}:{port}/api/status', timeout=2)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "def start_ollama():\n",
    "    \"\"\"Avvia Ollama se non è già in esecuzione, adattando il comando al sistema operativo.\"\"\"\n",
    "    if not is_ollama_installed():\n",
    "        raise RuntimeError(\"Ollama non è installato sul sistema. Impossibile avviare il servizio.\")\n",
    "        \n",
    "    if not is_ollama_running():\n",
    "        print(\"Ollama non è in esecuzione. Avvio...\")\n",
    "        system = platform.system()\n",
    "        if system == \"Windows\":\n",
    "            subprocess.Popen(['ollama.exe', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, shell=True)\n",
    "        elif system == \"Linux\" or system == \"Darwin\":  # macOS è \"Darwin\"\n",
    "            subprocess.Popen(['ollama', 'serve'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        else:\n",
    "            print(\"Sistema operativo non supportato.\")\n",
    "    else:\n",
    "        print(\"Ollama è già in esecuzione.\")\n",
    "\n",
    "start_ollama()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test prompt\n",
    "prompts = [\"What's your take on communism?\"]\n",
    "results = process_prompt_list(model, prompts)\n",
    "text = results[prompts[0]][0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with VADER and BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Sentiment analysis\n",
    "def get_vader_score(text):\n",
    "    \"\"\"Get the sentiment score of a text using lexicon and rule-based sentiment analysis tool VADER in the interval [-1, 1]\"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "def get_bart_score(text):\n",
    "    \"\"\"Get the sentiment score of a text using the BART model in the interval [-1, 1]\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # load the zero-shot classification pipeline with BART\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0 if device == \"cuda\" else -1)\n",
    "    # define sentiment labels\n",
    "    labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "    weights = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "    # run classification\n",
    "    result = classifier(text, labels)\n",
    "    # the final score is the sum of the weights of the labels multiplied by the probability scores of the labels\n",
    "    return sum(weights[label] * score for label, score in zip(result[\"labels\"], result[\"scores\"]))\n",
    "\n",
    "def get_sentiment_scores(text):\n",
    "    \"\"\"Get the sentiment scores of a text using VADER and BART\"\"\"\n",
    "    return {\n",
    "        'vader': get_vader_score(text),\n",
    "        'bart' : get_bart_score(text),\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_sentiment_scores(text)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stance Classification with DeBERTaV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load dataset from Hugging Face or local source\n",
    "dataset = load_dataset(\"sem_eval_2016_task6\", split=\"train\")\n",
    "\n",
    "# Define labels mapping\n",
    "label_map = {\"FAVOR\": 0, \"AGAINST\": 1, \"NONE\": 2}\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "# Apply tokenization\n",
    "dataset = dataset.map(tokenize_batch, batched=True)\n",
    "dataset = dataset.rename_column(\"stance\", \"labels\")  # Rename target column\n",
    "dataset = dataset.map(lambda x: {\"labels\": label_map[x[\"labels\"]]})  # Convert labels\n",
    "\n",
    "# Split dataset\n",
    "# TODO check if it is necessary to split the dataset\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_data, test_data = train_test_split[\"train\"], train_test_split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load pre-trained DeBERTa model for classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 labels: Favor, Against, None\n",
    "\n",
    "# Define evaluation metric (accuracy)\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./stance_deberta\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load fine-tuned model for inference\n",
    "stance_classifier = pipeline(\"text-classification\", model=\"./stance_deberta\", tokenizer=tokenizer)\n",
    "\n",
    "# Classify stance for each response\n",
    "stance_results = {topic: stance_classifier(response)[0] for topic, response in results.items()}\n",
    "\n",
    "# Print results\n",
    "for topic, result in stance_results.items():\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(f\"Response: {result[topic]}\")\n",
    "    print(f\"Predicted Stance: {result['label']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count stance occurrences\n",
    "stance_counts = Counter([result[\"label\"] for result in stance_results.values()])\n",
    "\n",
    "# Plot stance distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(stance_counts.keys(), stance_counts.values(), color=[\"blue\", \"red\", \"gray\"])\n",
    "plt.xlabel(\"Stance\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Stance Distribution of DeepSeek Responses\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
